网络监视器的体系结构

摘要
    本文描述了一种同时监控多种协议的系统。 它执行全线速捕获并实现在线分析和压缩，记录感兴趣流的数据并且不会丢失信息。我们许诺在磁盘带宽，CPU容量和数据减少之间的系统中保持平衡，以便以全线速执行监控。我们详细介绍了该体系结构，并测量了示例实现Nprobe的性能。
    
索引-  被动网络监控，全线速捕获，多协议分析

一，导言
    目前，用于网络被动监视的技术有限地使用作为每个TCP/IP流的一部分存在的状态信息。虽然入侵检测系统（IDS）经常从重建的数据流执行模式匹配，但实时状态的处理（和建模）却很少见。通过使用TCP/IP和更高级别应用程序的状态信息，可以推断网络属性，例如数据包丢失[1]和往返时间[2]，从而实现从终端揭开网络和传输层的特征的系统应用。
    我们提出了一种网络监控架构，目前能够与1Gbps全双工网络连接。我们的方法是执行多协议分析; 这需要从线路捕获所有数据。应用程序，传输和网络协议的集成分析允许研究它们的交互。此外，传输和网络状态的关联允许数据减少，将捕获到磁盘问题转换为在线处理和磁盘带宽之间易于处理的权衡。该系统已在运营商级网络上成功运行，并且仅使用商用硬件，具有经济的大规模部署的潜力。

A.相关工作
    被动网络监控已经出现了许多以前的方法。 网络速度的增长催生了方法的演变。 监控的示例包括基于数据包捕获的内核实现的示例，例如packetfilter [3]和tcpdump / libpcap [4]。 专业硬件已经在OC3MON [5]以及最近的DAG [6]等工作中得到了重要应用，其部分数据包捕获率高达10 Gbps。 此外，该领域的新项目正在进行中：欧洲SCAMPI [7]，一种数据采集的架构设计; 和MAGNeT [8]。 MAGNeT是一种用于将内核事件导出到用户空间的监视机制。 该项目使用内核/传输堆栈的工具来深入了解网络特征和应用程序行为。 但是，我们希望MAGNeT等系统在部署时会因操作系统的需要而受到限制。
    除了我们的方法，允许多协议分析的其他监控系统是Windmill [9]和BLT [10]。 Windmill是一种探测体系结构，旨在重建应用程序级协议并将其与底层网络协议相关联。 BLT [10]提取完整的TCP和部分HTTP级别的数据包跟踪。与我们的设计相比，BLT使用磁带存储，而Windmill在收集期间导出数据。这些方法需要权衡（复杂性和有限的捕获率），但允许连续收集跟踪信息。从一开始我们的架构的设计约束是磁盘容量（尽管这可以通过高数据缩减率来缓解）。提供足以记录二十四小时痕迹（因此任何昼夜交通周期）的磁盘容量可能足以满足大多数研究的需要，并且可以通过重复跟踪来处理更长的周期性流量变化。此外，通常的载波实践是以远低于最大速率运行传输容量[11]，这意味着虽然我们的系统必须处理全速率流量突发，但在长捕获运行过程中，全速率捕获并不意味着总线速捕获。与Windmill和BLT一起，我们的方法采用了数据包过滤方案。 Windmill和BLT都实现了粗粒度数据丢弃基于内核/应用程序的数据包过滤，但相比之下，我们的过滤器体系结构相对简单并且用于不同的目的——支持可扩展性，如第II-D节所述。
    SNORT [12]等IDS也可以执行在线捕获和分析。 SNORT中的模块（例如stream4）允许检查分组状态并随后重新组装TCP流;重组模块具有处理多达64,000个并发流的理论能力。然而，基于签名/模式匹配的IDS是与多协议分析器非常不同的问题。我们的方法是执行TCP / IP和应用程序协议分析的工具，包括提供事件之间的时序关系; SNORT和其他IDS的目标是对所有数据包执行快速字符串匹配 - 仅捕获那些感兴趣的数据包。
    我们在这里介绍的工作是在[2]中提出的原型上绘制的，称为Nprobe。该监视器已用于多项研究，最近一项是对网络流量的研究[13]。
    本文的其余部分结构如下：第二部分详细介绍了我们的探针架构。第III节描述了我们的测试方法，流量准备和硬件设置，然后是我们的性能评估。最后，第四节讨论了我们的结果，第五节总结了论文。

二，架构    
A. 意图
    一个从刚开始就对我们设计产生约束的条件是我们希望从不同的层面捕获尽可能多的数据如网络栈：网络层，传输层，应用层。对于低速网络，我们的解决方案是近可能捕获到磁盘，由tcpdump类型的程序和大型的离线处理组成。 然而，磁盘带宽限制以及对改进时间戳的需求加上大量的数据减少以及通用CPU的增加速度需要提供了另一种方法。
   为了捕获网络，传输和应用层的所有数据，我们的方法包含了多个组件。 首先认识到网络数据存在相当大的冗余：在任何网络数据包内和网络数据包之间。 简单的无损压缩系统长期以来一直应用于改善链路层机制（如高速调制解调器）的性能 - 存在以相同方式压缩捕获数据的充足范围。 所需的CPU数量（与压缩程度成正比）与磁盘带宽之间存在明确的交易关系。 使用足够大量的数据可以消耗所有可用的磁盘和CPU资源，但是对于更适度的要求，限制因素倾向于PCI总线带宽。 
    鉴于丢弃是最佳压缩形式，通过执行仅捕获重要或感兴趣信息的提取，可以实现更多的压缩。 以这种方式，Web流量的捕获将仅涉及记录每个HTTP事务的最小TCP或IP头信息以及HTTP事务本身。 检索到的对象的数据也不相关并被丢弃，因为它是感兴趣的协议操作。 使用这种技术可以获得显着的压缩比例。
   显然，为了换取磁盘的原始带宽，我们的架构必须能够识别属于每个感兴趣的流的数据包（以便对它们执行压缩）。 在运营商规模上从重新组装的TCP流执行实时协议特征提取的架构被认为是这项工作的核心工作。 
   除了这种特定于应用程序的压缩之外，我们的方法还包括在多台机器之间进行带宽分割以共享工作负载的方法。 
   对于单个监视器，磁盘和可用CPU的带宽最终将成为瓶颈。 对于负载分配方法，我们建议任何特定IP主机 - 主机对负责的网络流量的硬上限 - 这将是单个监视器能够管理的带宽。 对于更广泛的公共互联网以及共同的研究和学术网络，我们认为这是一个可接受的设计决策。 这种拼版意味着我们的系统不适合更不寻常的情况，例如仅承载单一流量的10 Gbps网络[14]。
    在最坏情况下的分割方法中，来自单个IP主机 - 主机对的流量可以在多个监视器机器之间进行条带化。 这种方法简化为捕获到磁盘的问题，因为只能对网络，传输和应用程序流进行离线分析。 对于我们的方法，我们使用IP主机 - 主机对的两个地址的XOR'd乘积作为每个监视器的流量过滤器的输入。 过滤由网络接口执行，因此丢弃的数据包永远不会导致显示器的CPU或PCI或磁盘总线上的负载。
   这种方法意味着每个监视器将看到任何特定流中的所有流量，因此可以使它们承受上述网络层，传输层和应用层压缩的影响。 虽然很少见，但是在特别连接良好的主机之间检测到异常数量的流量时，我们进行的多次部署需要在监视机器之间进行一些负载平衡。 此过程很容易在监视器上检测为资源溢出，解决方案是为监视器计算更合适的过滤器表。 虽然IP地址被重新分配，使这个过程反复进行，但我们已经注意到这种迭代是在非常长的时间尺度（几个月）上，与新服务器和路由器的调试时间尺度相匹配。

B. 架构概述
    图1说明了我们架构的主要组件和布局。
     我们的实现基于GNU / Linux操作系统，经过修改以改进用于将数据日志文件写入磁盘的异步msync（）系统调用的行为。 我们还修改了网络接口卡（NIC）的固件，以提供第II-C.1节中描述的高分辨率时间戳和第II-D节中描述的包过滤能力。 
    我们的方法包括三个阶段：数据包捕获，数据包处理和数据存储。 阶段之间的缓冲容纳在分组处理和数据存储速率分组到达和变化突发性。
    从网络到达的数据包将被提供给我们放置在NIC固件中的简单过滤器; 通过过滤器的那些是带时间戳的，并且无需任何进一步处理即可传输到内核存储器接收缓冲池中。
    监视机器具有一个或多个接收缓冲池，每个接收缓冲池与一个用户级进程相关联并映射到其地址空间。 每个进程将接收缓冲池中保存的数据包呈现给一系列基于协议的模块，这些模块从每个数据包中原位提取所需数据。 所有模块完成数据包后，将返回缓冲区以供网络接口驱动程序重用。 模块通常被设计为以FIFO顺序处理和返回缓冲区，但有时保持缓冲区是有用的，例如当HTTP模块解析由于丢失而重新排序的流时。 如果可用网络缓冲器的数量低于阈值，则系统可以以最近最少分配的方式快速回收这种“保持缓冲器”。
   数据提取通常取决于先前数据包处理所提供的上下文。 该上下文保持为在处理时附加到每个数据包的状态。 提取的数据暂时存储为附加状态的一部分。
    当处理了相关系列的最后一个数据包时，保存在状态中的所有提取数据被复制到输出日志缓冲区中，一旦累积了足够的数据，就将其写入大型RAID磁盘。
    该体系结构采用特定于协议的模块，这些模块又定义应存储哪些数据以及应丢弃哪些数据。 因此，对于HTTP模块，通过丢弃数据对象来实现主要数据压缩。 当数据包在内存中时，模块使用CRC64散列指纹每个对象; 但是对象本身不会被保存。 哈希允许我们识别对同一数据对象的引用，即使它们使用不同的URL。 

C. 捕获
    为处理系统提供数据有两种方法。 第一种是在线捕获数据：数据从线路直接提供给处理系统。 这是标准操作模式。 第二种方法是为处理系统提供离线捕获的跟踪。
    1）在线捕获：在此模式下，修改的NIC固件在每个接受的数据包前面加上由卡上的时钟生成的到达时间戳。 以这种方式，消除了由于分组到达和处理之间的等待而导致的不准确性。
    我们实现当前使用的NIC提供的时钟提供的时序精度和精度约为1毫秒，尽管这很容易受到温度漂移的影响。 在处理数据包时，NIC生成的时间戳会定期与系统时钟进行比较。 当前的NIC时钟频率是根据两者的经过时间计算得出的，并且其当前偏移量是实时记录的。 当从接收缓冲池中抽取数据包时，这两个参数用于计算每个参数的准确实时到达标记。 NIC生成的时间戳和系统时钟之间的周期性比较基于两者的少量重复读数，以便识别和消除由于干预中断处理而可能出现的不准确性。
    以这种方式生成的时间戳具有一或两微秒的相对精度，并且绝对精度由系统时钟的精度确定 - 通常在几毫秒内，使用网络时间协议[15]。 虽然这种精度不足以准确测量1 Gbps及以上网络带宽的背靠背数据包的序列化时间，但它与带宽为100 Mbps或更小的最小规模数据包的序列化时间的顺序相同 带宽为1 Gbps的（512字节）数据包。
    2）离线文件输入：在这种操作模式中，作为从接收缓冲池中提取数据包的替代方法，处理系统可以从tcpdump格式的跟踪文件中读取它们。 此工具是为了开发目的而提供的，但是能够将数据提取和分析应用于使用tcpdump收集的跟踪也很有用（当然，tcpdump的限制仍然适用）。
    从tcpdump跟踪文件读取数据包的能力也用于调查数据处理失败，在这种情况下，有问题的数据包被转储到错误日志中。 这种系统的价值在于可以在闲暇时分析出现的任何异常情况; 导致自动生成数据包跟踪的异常，然后可以在开发反馈循环中用作输入，以编程代码以应对观察到的异常。
    选择tcpdump格式是因为它允许使用tcpdump检查错误日志文件，特别是因为数据包过滤器可用于选择数据包以便按错误类型显示或提取（可能与其他标准结合使用）。

D.可扩展性
   必须认识到，由于交通量的增加，监控系统跟上数据包到达的能力在某些时候是不够的。 
   这同样适用于可用的处理器周期，存储器访问时间以及总线和I / O带宽。
    我们实现的可扩展性基于跨多个进程的数据包条带化，可能在多个监视器上运行; 虽然单个基于PC的监视器的容量可能低于专用硬件的容量，但它们相对较低的成本使得监视集群的使用成为一个有吸引力的主张。 可选地，条带化可以用作粗粒度数据丢弃的形式，以便通过丢弃总条带的子集来收集总网络流量的样本。
    该过滤器使用基于分组的XOR'd源和目的地IP地址的n值散列 - 因此在n个进程之间分配流量，每个进程处理总数的特定聚合双向子集。此过滤器在每个监视器NIC的固件中实现：接受的数据包被传输到与单个进程关联的接收缓冲池中;被拒绝的数据包被丢弃，因此不会将负载放在监视器的PCI总线上。在采用多处理器监视器机器的情况下，每个处理器运行一个进程，从而利用流对处理器的亲和性。如果所需的吞吐量超过单台计算机上可能的吞吐量，则可以部署多个监视器，从而形成监视集群。
    图2说明了每个监视器和多个监视器使用多个CPU的集群。为了实现流之间关系的全面分析，需要额外的离线处理来合并多个监视系统捕获的数据。
    我们的方法所施加的限制是监视器无法处理超出其容量的任何单个主机 - 主机流的流量。 为了证明这种限制，我们的公共运营商部署经验涉及一个主要的英国ISP，它提供拨号和电缆调制解调器接入（例如56kbps  -  2Mbps）。 由于对最后一跳链路容量的限制，一种将主机 - 主机数据速率限制在低于任何单个监视器系统可以舒适处理的架构的架构是一个非常合理的设计决策。 这样的限制将阻止监视器出现更多异常情况，例如仅承载单一流量的10 Gbps网络[14]。
